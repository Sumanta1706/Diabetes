# Diabetes
## Aims & Objective
Diabetes mellitus is a chronic disease that occurs when one’s pancreas no longer able to produce enough insulin.  The  long-term  hyperglycemia  during  diabetes  causes  chronic  damage  and dysfunction  of  various  tissues,  especially  the  eyes,  kidneys,  heart,  blood  vessels,  and  nerves. Nowadays, diabetes is a major public health challenge and a worldwide problem. This study will introduce how to use medical data to analyze the relation between medical indexes and diabetes with machine learning tools. It may be helpful to doctors as a screening tool.
## Method 1: K-Nearest-Neighbours (KNN)
### Model building: 
We’ll begin by applying the k-nearest neighbors method of classifying patients by their similarity to other patients. For this method (and all subsequent methods), we’ll start by separating the data set into “training” and “test” sets. We’ll build our model based on the relationship between the predictors and the outcome on the training set, and then use the model’s specifications to predict the outcome on the test set. We can then compare our predicted outcomes to the test set’s actual diabetes status to give us a measure of model accuracy. For my exercises, I’ll use the sample.split function from the caTools package.
### Validation:
For k-nearest neighbors, we compute the outcome for each test case by comparing that case to the “nearest neighbors” in the training set. The assigned outcome depends on how many of these neighbors you decide to look at; the majority class of the three closest neighbors may be different than the majority class of the five closest neighbors.
To ensure we use a number for k that gives better model performance, I performed a two-part cross-validation. First, I varied the possible values for k from 2 to 10; second, I repeated the splitting of the data into training and test sets 100 times to ensure a robust estimate of model performance for each k. I used the knn function within the class package and computed model accuracy on the test set for each fold.
### Evaluation: 
From this analysis, we can see that k-nearest neighbors performs better for somewhat larger values of k, with performance reaching a maximum of about 75% classification accuracy. Though there is still some variance depending on the exact data split, using 9 or 10 neighbors seems to yield fairly stable model estimates on the test set.
At first, we have to install some of the packages like ‘mlbench’, ‘caTools’ etc. Then after, split the data set into 80% and 20%, and then we checked the accuracy for all of the corresponding neighbors. We can see that for k = 10, we have the best accuracy. 
